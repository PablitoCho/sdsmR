---
title: "Statistical Downscaling in R- Blogsville Example"
author: "Lee Richardson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Downscaling in R- Blogsville Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", fig.height = 7, fig.width = 7)
```

# Introduction

In this document, we will talk through how to perform statistical downscaling, such as the kind performed by the Statistical Downscaling Model (SDSM) tool, in the R language. Statistical downscaling is a method of relating large scale variables such as those from General Circulation Model (GCM) output, or Re-Analysis data, to local, usually pointwise variables. Statistical downscaling provides a method for performing local impact studies, such as assessing the risk of flooding in a particular city, under different climate change scenario's. At a high level, what's being done is building a statistical model of the relationship between these large scale variables (such as surface temperature inside a gridbox), and and a local variable using re-analysis data, and then using this relationship to generate scenario's for how the local variable responds to various climates. 

We will walk through the Blogsville eample used in the SDSM manual, and show how you can perform these calculations in the R language. Also, we will show how you can use various functions provided by the sdsmR package to speed up process. 

# Exploratory Data Analysis 

## Accessing the data
Before we do anything, we must load the Blogsville data into our R workspace. Loading data into R is both art and science, and there isn't an exact recipe for loading every dataset you will encounter into R. For this reason, we will include instructions on how to load the Blogsville dataset into R manually in a supplement of this document, to get a flavor for how this is done. For now, we use the blogsville dataframe which comes pre-installed with the sdsmR package to walk through the steps of statistical downscaling. 

The fact that blogsville is a dataframe object in R is very important, and this allows lots of the other commands and functions to work properly. If you are working through this tutorial with a different dataset, the best first step to take would be to combine all of your data into one dataframe object. This will allow you to work through the tutorial smoothly and use all of the functionality of sdsmR. For tips and an example of how to load data into a dataframe, see the supplement of this document. 


```{r}
# Load the sdsmR package into memory, and view what the 
# blogsville dataframe looks like. 
library("sdsmR")
head(blogsville)
```

The above code loads in the sdsmR package, which includes a dataframe named blogsville. For detailed description of the data frame and the variables included, use the help command `?blogsville`. 

## Example plots 

Whenever building a statistical model, the first step is to visualize the data and look into the relationships between predictor and predictand variables. For those familiar with the SDSM tool, the first stage of the analysis is usually the Screen Variables section. In SDSM, the three main plots for exploratory data analysis are:

1. The explained variance due to each predictor on the predictand, by month. 
1. Correlation between all predictor variables, by month. 
1. Scatterplot of an individual predictor and predictand. 

Creating these first two plots is a bit tricky. Due to this, we have created a function, called generate_table, which can be used to create these informative tables. The structure of this function is that you input a filepath, dataframe, and specify the predictand variable, and as an output you will recieve two pdf files with these two tables, in the filepath specified as an input. For example, here is how I would save the explained variance and correlation tables for maximum temperature into my home directory:

```{r, eval=FALSE}
generate_table("/home/lee/example", blogsville, tmax)
```

Next, let's consider how we can create these scatterplots. If you look at the explained variance table created by the generate_table function, you will see that out of all the predictor variables, humidity (humxx) has the highest correlation with maximum temperature. From this, let's look at how to create a scatterplot of humidity against maximum temperature:

```{r, fig.height = 6, fig.width = 6, fig.cap = "Scatterplot which displays the relationship between daily humidity and maximum temperature. We see from this plot that there is a fairly strong, positive relationship between these two variables."}
plot(blogsville$humxx, blogsville$tmax, xlab = "Humidity", 
     ylab = "Maximum Temperature", main = "Humidity vs. Maximum Temperature", 
     cex = .1)
abline(lm(tmax ~ humxx, data = blogsville))
```
<!-- bplot.xy(blogsville$humxx, blogsville$tmax) -->

So now we have a way to repdresent all of the SDSM plots from screen variables in R. One thing to note about R is that it has very powerful graphics capabilities, which allow you to create many advanced, publication worthy figures. To give a flavor of this, we will show some of the other kinds of plots you can create with the blogsville data. First, let's look at how to visualize how maximum temperature changes throughout the dataset, by plotting it against time. 

```{r, fig.height=7, fig.width=7}
# Pull out the year our of the dates column in R, and 
# then add a variable to the dataframe which represents 
# the the three different decades. 
dates <- as.numeric(format(blogsville$dates, "%Y"))
period <- rep(1, length(dates))
period[dates < 1970] <- 1
period[dates < 1980 & dates >= 1970] <- 2
period[dates < 1990 & dates >= 1980] <- 3
blogsville$period <- period

# First, set R's output window to contain 3 panels 
# for plotting. Next, plot the time series of maximum temp
# in each of the three different period we specified above. 
# Make sure all of the scaled are the same with the ylim command. 
par(mfrow = c(3, 1))
plot(tmax ~ dates, data = blogsville[blogsville$period == 1,], 
     cex = .2, col = "red", ylim = c(0, 30), 
     main = "Daily Maximum Temperature in Blogsville over
     three different decades")
plot(tmax ~ dates, data = blogsville[blogsville$period == 2,], 
     cex = .2, col = "red", ylim = c(0, 30))
plot(tmax ~ dates, data = blogsville[blogsville$period == 3,], 
     cex = .2, col = "red", ylim = c(0, 30))

# Remove the period variable from the blogsville dataframe
blogsville <- blogsville[, 1:8]
```

# Model Calibration
Next, we will see how to use the blogsville data frame in order to build a statistical model. This is the same stage as the Calibrate model section of the SDSM tool, and we will use the same predictor variables that the SDSM manual uses in their blogsville example. We also follow the convention blogsville example in the SDSM manual by splitting our dat into training and testing datasets, using the former for model building and the latter for validation. 

First, we will show how to fit a annual model using the blogsville example, and that the values derived from this are identical to what you would have obtained from the SDSM tool. However, we can see from the above figure that maximum temperature is very dependent on time, and we will see in the next section how to fit both monthly and seasonal models, as well as include an autoregressive term. 

```{r}
# Split the dataset in half into training and testing 
# sets. This is the same split used by Wibly in the SDSM
# manual which splits the data into 1960-75 and 1976-90 
# time periods
train <- blogsville[1:5478, ]
test <- blogsville[5479:10957, ]

# Fit a linear model to the tmax predictand using all of the 
# predictor variables from blogsville. Note that we are using the 
# training data defined above in order to build this model. 
blogsville_annual_model <- lm(tmax ~ uxx + vxx + zxx + xx500 + humxx, 
                              data = train)

# Print the R Squared and Standard error from the blogsville
# annual model. Note that these numbers are identical to those 
# produced by SDSM!
summary(blogsville_annual_model)$r.squared
summary(blogsville_annual_model)$sigma
```

## Monthly and Seasonal Models 
Now that we have seen how to fit our annual model using R, let's delve into how we can create more complicated models. In SDSM, one of the most important options is the ability to specify whether you want annual, monthly, or seasonal models. If you choose a monthly or seasonal model, what SDSM does is fit a seperate linear model for each time period. For example, if you select monthly, SDSM will return a seperate linear model for january, february, march, etc... Carrying out this step in R is a bit complex, but certainly feasible. We will first show you how to manually fit monthly/seasonal models in R, then we will introduce the calibrate_model function which comes with the sdsmR package, which will automate this step for you!

```{r}
# Get a list of the months in our dataframe and add a column 
# which contains month into our blogsville dataframe. Note that we will
# denote month as a factor in R, which is the normal way that categorical 
# variables are stored. 
months <- unique(format.Date(blogsville$dates, "%m"))
blogsville$month <- factor(format.Date(blogsville$dates, "%m"))

# Using this month variable, create another column which represents
# which season each data-point corresponds to. 
season <- rep(NA, nrow(blogsville))
season[blogsville$month %in% c("12", "01", "02")] <- "winter"
season[blogsville$month %in% c("03", "04", "05")] <- "spring"
season[blogsville$month %in% c("06", "07", "08")] <- "summer"
season[blogsville$month %in% c("09", "10", "11")] <- "fall"
blogsville$season <- factor(season)

# Next, set up a data-frame which will hold the summary statistics 
# for each monthly linear model. Then loop through each month, 
# subset the data frame so it only contains data for the 
# individual month, fit a linear model using this monthly dataset, 
# and finally record the r squared value along with the residual standard error. 
# Finally, note that we are storing all of these models in a list
# called month_mods, so you can access and explore the individual 
# models after calibration. 
month_mods <- list()
month_r2 <- data.frame(month = months, r.squared = NA, se = NA)
count <- 0
for (m in months) {
    count <- count + 1
    month_dataframe <- subset(blogsville, month == m)
    month_lm <- lm(tmax ~ uxx + vxx + zxx + xx500 + humxx, data = month_dataframe)
    month_mods[[m]] <- month_lm
    month_r2[count,"se"] <- round(summary(month_lm)$sigma, digits =  3)
    month_r2[count,"r.squared"] <- round(summary(month_lm)$r.squared, digits =  3)
}                 

# Perform the same calulations used in the month above, except using 
# season instead. 
season_mods <- list()
season_r2 <- data.frame(season = unique(blogsville$season), r.squared = NA, se = NA) 
count <- 0
for (s in unique(blogsville$season)) {
    count <- count + 1 
    season_dataframe <- subset(blogsville, season == s)
    season_lm <- lm(tmax ~ uxx + vxx + zxx + xx500 + humxx, data = season_dataframe)
    season_mods[[s]] <- season_lm
    season_r2[count,"se"] <- round(summary(season_lm)$sigma, digits =  3)
    season_r2[count,"r.squared"] <- round(summary(season_lm)$r.squared, digits =  3)
}

# Print the results of the r squared and residual standard 
# error terms. Compare these with the output that SDSM produces, to see
# that they ae indeed the same. 
print(month_r2)
print(season_r2)
```

Above is a way to compute a seperate linear model for either each month, or each season. Note that we have stored these models inside a list, which is one of R's fundamental data structures. To access one of these models, simply type `season_mods$winter` or `month_mods[[1]]`, and to view a summary of one of these models, type `summary(season_mods$winter)`. 

An alternative to using the code above to store linear models would be the calibrate_model function, which comes with the sdsmR package. It works the same way as above, storing each model in a list. Notice how the output of this model is similar to the way we calibrated the model above, and we can pull this informartion out with the summarize models function

```{r}
# Fit a blogsville model using the calibrate model function. Note that 
# the -which(colnames(test) == "pcrp") command is just making sure 
# that we are not using precipitation as a predictor for maximum 
# temperature. 
blogsville_mod <- calibrate_model(train[, -which(colnames(train) == "pcrp")], 
                        y = "tmax", model_type = "annual")
blogsville_summary <- summarize_models(blogsville_mod)
print(blogsville_summary)
```

# Weather Generator 
Now that the have our models built, we will show how to produce the equivalent of the weather generator section of the SDSM tool. Generally, this section is used to verify calibrated models, so we will use our testing data-set defined above to generate the local weather and see how well the model performs. Recall that we split the blogsville data-set into a training and testing halves, the first form 1960-1975, and the second from 1976-1990. We build our model using the training portion, and we will now evaluate our model using the test set. 

SDSM adds white noise, ie: a normal random variable, onto all of its predictions in order to create ensembles of weather predictions. The reasoning behind generaing these ensembles is to closer match the variance of the observed and downscaled distributions. Next, we will show how to generate these ensembles manually using our annual model, and then show how to use the generate_weather function which comes with the sdsmR packge to perform the same calculations.  

```{r, fig.cap= "testing", fig.height = 7, fig.width = 7}
# Use the predict.lm function in R with our annual model, 
# along with the test data. This will output a numeric 
# vector of predictions for each observation in the test dataset. 
# Note that if we don't provide new data, the default is that 
# predict.lm will return the fitted values for this model. 
weather <- predict.lm(blogsville_mod$annual, 
                newdata = test[, -which(colnames(test) == "pcrp")])

# Add a normal random variable to the predictions based 
# on how many ensembles are desired. The paramaters for the normal 
# random variable are mean 0, and standard error equivalent to 
# the standard error from the linear model. 
# In this case, we will create 20 different ensembles, 
# the default in SDSM. 
num_ensembles <- 20
num_predictions <- nrow(test)
sigma <- blogsville_summary$annual$standard.error
white_noise <- matrix(rnorm(num_predictions * num_ensembles, 
                    mean = 0, sd = sigma), ncol = num_ensembles)
ensembles <- white_noise + weather 
head(ensembles[, 1:6])
``` 

Similar to the other sections, we have created an R function, 
called generate_weather, which mimics the functionality of the above 
code chunk for all models you can generate with calibrate_model. 
With this function, you can use the output from the 
calibrate_model function and a new dataframe as inputs,  
and you will recieve a dataframe with the dates, exact predictions, 
and specified number of ensembles. 

```{r} 
blogsville_preds <- generate_weather(models = blogsville_mod, 
                        new_dataframe = test[, -which(colnames(test) == "pcrp")], 
                        uncertainty = "ensemble", num_ensembles = 20)
head(blogsville_preds[, 1:6])
```


## Prediction Intervals 
Next, we will show a different way of quantifying uncertainty in our downscaled predictions. Instead of adding random noise to the pointwise prediction generated from the calibrated model, we use a prediction interval, which comes straight from the regression equation. The reason to choose the interval approach instead of the ensembles is that it gives coverage guarantees: IE: approximately 95 percent of the prediction intervals will contain the true value. 
In the following section, we show the R code which generates both the predictions and corresponding prediction intervals. Note that the actual predictions are the same whether you're generating predictions intervals or using ensembles (check this with `all.equal(as.numeric(weather), blogsville_preds$predictions)`), but what is different is how we are quantifying the uncertainty. 

```{r}
# Specify the interval = "prediction" option in predict.lm to 
# obtain prediction intervals for this new dataset. Then, 
# combine these intervals with the true observations. 
observed <- test[, "tmax"]
weather_intervals = predict.lm(blogsville_mod$annual, 
                        newdata = test[, -which(colnames(test) == "pcrp")], 
                        interval = "prediction")
prediction_results <- cbind(observed, weather_intervals)

# This just demonstrates which percentage of observations our 
# intervals are trapping. The number is very close to  95%. Note 
# that one of the observations in the blogsville data-set is an NA, 
# which is why we need to use the na.rm = TRUE command in the sum. 
upper <- prediction_results[, "upr"]
lower <- prediction_results[, "lwr"]
trapped <- (observed < upper) & (observed > lower) 
sum(trapped, na.rm = TRUE)/length(trapped)
```

Finally, let's look at what the two methods for generating uncertainty look like against one another, for the year 1975 in the Blogsville dataset. 

```{r, fig.show='hold'}
# Ensembles -------------------
cols <- rainbow(20)
plot(test$dates[1:365], weather[1:365], type = "l", 
     ylim = c(0, 35), lwd = 3, 
     main = "Predicted vs. Observed Weather 1975 in Blogsville", 
     xlab = "Date", ylab = "Maximum temperature")
for (i in 1:20) {
    lines(test$dates[1:365], ensembles[1:365, i], 
         col = cols[i], lwd = .2)
}
lines(test$dates[1:365], weather[1:365], type = "l")
points(test$dates[1:365], test[1:365, "tmax"], cex = 1, 
       col = "black", pch = 16)

# Prediction Intervals --------------
plot(test$dates[1:365], prediction_results[1:365, 2], 
     type = "l", lwd = 3, ylim = c(0, 35), col = "red" , 
     main = "Prediction Intervals 1975 in Blogsville", 
     xlab = "Date", ylab = "Maximum temperature")
lines(test$dates[1:365], prediction_results[1:365, 3], 
      col = "purple", lwd = 2) 
lines(test$dates[1:365], prediction_results[1:365, 4], 
      col = "purple", lwd = 2)
points(test$dates[1:365], test[1:365, "tmax"], cex = 1, 
       col = "black", pch = 16)
```

As we can see, both options do a fairly good job giving a range of weather to expect in observations not used by the training dataset. However, we do notice that niether does a great job capturing the extremely hot days, which occur around the middle of July. 

# Scenario Generation 
Finally, we arrive at the main section where we use GCM output to downscale local variables. In this case, we are using predictors from the HadCM3 experiment, which are the same one's that were included in the Blogsville SDSM manual. 
Note that this process of generating scenario's from climate model scenario's is statistically the same as the generating weather function we used in the previous section. Howeve
r, instead of using a test set of the NCEP data, we are using climate model output. Because of this, we can use the same generate_weather function to generate these different scenario's. The input requirements are that you must have a dataframe with the same predictor variable names that were used in the model calibration stage. 
Below we will generate the scenario's from both 1961-1990 and 2070-2099, along with thir corresponding uncertainty. We will show that, as expected from climate change, the maximum temperatures are expected to be higher in Blogsville in 2070-2099 compard with 1961-1990. Note that similar to the blogsville data, the climate model output comes with the sdsmR package, so we can just load it into R's workspace automatically. 


```{r, fig.show='hold'}
# Load in the climate model output for the two time periods. Recall
# this comes pre-insalled with the sdsmR package. 
data(weather_1960)
data(weather_2070)

# Generate the wetaher using the Blogsville monthly model. 
# Note that we are using the monthly model for weather demonstration 
# since it was the same used in the SDSM model, and there is a clear 
# dependence between time of year and maximum temperatures. 
blogsville_monthly_mod <- calibrate_model(train[, -which(colnames(train) == "pcrp")], 
                                y = "tmax", model_type = "monthly")
preds_1960 <- generate_weather(blogsville_monthly_mod, 
                               new_dataframe = weather_1960)
preds_2070 <- generate_weather(blogsville_monthly_mod, 
                               new_dataframe = weather_2070)

# Plot the two figures side by side 
par(mfrow=c(1,2), oma = c(0, 0, 2, 0))
plot(preds_1960$dates[1:(365*3)], preds_1960$predictions[1:(365*3)], 
     type = "l", ylim = c(0, 35), xlab = "", ylab = "Maximum Temperature")
abline(h = mean(preds_1960$predictions), col = "red", lwd = 2)
plot(preds_2070$dates[1:(365*3)], preds_2070$predictions[1:(365*3)], 
     type = "l", ylim = c(0, 35), xlab = "", ylab = "")
abline(h = mean(preds_2070$predictions), col = "red", lwd = 2)
mtext("Temperature Max Scenario in Blogsville for Time Periods 
      1961-1963 and 2070-2072", side = 3, outer = TRUE)
```

In the above plot, we see the first three years of generated maxiumum temperatures for each one of our two scenario's. We also plot the mean maximum temperature over all years in each scenario. From the plots, two things jump out. First, the average maxiumum temperature is clearly higher in the second scenario than the first. And second, this seems to be due to an overall shift, since the both the minimum and maximum temperatures for the earlier scenario are lower. 

# Conditional Models 
Another important function of the SDSM tool is the ability to fit conditional models. The example used for these models in the manual is for precipitation. Since there are many days with no precipitation, SDSM fits a two stage model. First, we will create a new binary variable, $W_{i}$, which equals 1 if there is precipitation on day i, and 0 if there isn't.  Next, we fit a linear regression model to $W_{i}$ using a set of predictor variables, and call this the first stage model. 
For stage two, we subset the data we are using only to include days in which there is precipitation (ie: $W_{i}$ = 1). Next, call the amount of precipitation on day i $P_{i}$. Using the same predictor variables we used to fit the linear regression model in stage one, fit a linear model to $P_{i}$. And that's it, here is the code which performs those first two steps:

```{r}
# Create a 0/1 variable which indicates whether or not there 
# was precipitation on this day. Make sure to do this on both 
# the training AND test dataset
train$wetday <- ifelse(train$pcrp > 0, 1, 0)
test$wetday <- ifelse(test$pcrp > 0, 1, 0)

# Fit a linear model to the wetday variable created above. 
prec_step1 <- lm(wetday ~ uxx + vxx + zxx + xx500 + humxx, 
                 data = train)

# Subset the training data to only include data in which there
# was a wet day. Then, use this dataset to fit a linear model based 
# on the amount of precipitation
prec_train <- subset(train, wetday == 1)
prec_step2 <- lm(pcrp ~ uxx + vxx + zxx + xx500 + humxx, 
                 data = prec_train)
```

Now that we have our two linear models calibrated, we move onto generating weather and scenario's with these models. First, we will use our first linear model, `prec_step1`, from above to make a prediction for $W_{i}$. Note that we in SDSM, this value is truncated to be between 0 and 1. For our purposes, we won't need to include this step because, as we will see, truncating this value between 0 and 1 won't affect the prediction. Next, a uniform random number between 0 and 1, call this $U_{i}$, is generated. If $U_{i}$ is greater than $W_{i}$, then we predict there will be no precipitation. If it's smaller, then we will predict that there will be precipitation on this day. 

If we do predict that there will be precipitation on day i, then we will use our second model in order to predict how much precipitation there will be. Below is the R Code which executes these steps to make predictions for the blogsville testing dataset. 

```{r}
# Define a function to use the two calibrated linear models 
# in order to predict the amount of precipitation on a given day. 
# This function will be used below in order to make predictions 
# for all of our days in the testing dataset. 
predict_row <- function(lm1, lm2, row) {
    # Use the first linear model to predict whether or not there
    # will be a wet day
    wet_day <- predict(lm1, row)
    rand <- runif(1)

    # Determine how much precipitation based on the corresponding prediction.
    # If it's a wet day, determine the amount. However, if it's a non wet day,
    # then return a 0
    if (rand < wet_day) {
        precip_amount <- predict(lm2, row)
    } else {
        return(0)
    }
}

# Testing this function on a subset of models
cond_preds <- vector(mode = "numeric", length = 0)
for (i in 1:nrow(test)) {
    prediction <- predict_row(prec_step1, prec_step2, test[i,])
    cond_preds <- c(cond_preds, prediction)
}
```

Similar to other sections, we have included a way to fi conditional models as part of the sdsmR package. We implement this by specifying the `process = "conditional"` option included in the calibrate_model function. Then when you're using a conditional model fom this function, generate_weather will recognize this, and the predictions will be based on the conditional model algorithm explained above. Next, we show how to fit a conditional model using the sdsmR functions mentioned above, and then graphically display what these predictions look like. 

```{r}
    # Calibrate the conditional model using the calibrate model function. 
    # Note that we are using the blogsville[, -7] argument as our data
    # because we don't want to include tmax as a predictor variable.
    blogsville_cond_mod <- calibrate_model(dataframe = train[, -7], y = "pcrp",
                               model_type = "monthly", process = "conditional")

    # Using the calibrated conditional model, generate the weather for 
    # the testing dataset. Note that the predictions will come out and 
    blogsville_cond_preds <- generate_weather(models = blogsville_cond_mod, 
                                              new_dataframe = test)
    
    # Plot the conditional predictions 
    prec_observed <- test[, "pcrp"]
    plot(blogsville_cond_preds$dates, prec_observed, cex = .2)
    lines(blogsville_cond_preds$dates, blogsville_cond_preds$predictions, 
          lwd = .2)    
```


# Summary 

In this document, we've shown how to implement the main functionality of statistical downscaling using the R programming language. We have replicated the functionality of the four main SDSM functions:

1. Screen Variables
2. Calbrate Model
3. Weather Generator
4. Scenario Generator

While building the tool, we've taken great care to include all of the options that are familiar to users of the SDSM tool, such as fitting monthly/seasonal models, including an autoregressive term, generating the same tables, etc..

Throughout the document, we have shown how to obtain the SDSM outputs in two different ways. First, we've shown how you can produce these results with raw R code, which could be mimicked for particular use cases just by modifying a few lines of code. Next, we've shown how to use the functions provided by the sdsmR package to obtain the identical results. The reason we have taken this approach is because we believe there is value in both approaches. If a user wants to get something done quickly, or just doesn't want to type out the same lines of code over and over, using the pre-made functions is a good approach. However, if someone wants to really understand the process of statistical downscaling, and all of the steps involved, going through the actual code and trying to repeat it for your unique problem is a great way to learn.

To go a bit further in understanding what's happening under the hood with statistical downscaling, all of the code used is posted online at https://github.com/leerichardson/sdsmR

# Supplement: Loading Data

Before you can perform any of the calculations mentioned in the vignette, you must first have the data loaded into R's workspace as a dataframe. Generally speaking, this task is different for most data-sets you encounter, so we didn't decide to include it in the main sections of this document. The idea behind a dataframe is that every row is an observation, and every column is a variable. Once the data is into this form, executing statistical functions and algorithms becomes much simpler and reproducible for others. 

To give a flavor of how to transform a dataset into a dataframe, we are including the code and steps used to download the example blogsville dataset from the SDSM web page and convert it into a dataframe. While future situations most likely won't be identical to this, hopefully some of the ideas can be useful when you need to convery your data-set into a dataframe. 

## Blogsville Data 

First, you can download the Blogsville data at the following link on the SDSM homepage http://co-public.lboro.ac.uk/cocwd/SDSM/blogsville.html. This data-set has four different directories:

* gcmx1961-90. GCM output from five different predictor variables from 1961-1990. Ultimately used to generate scenarios for the desired local variables in this time period.  
* gcmx2070-99. GCM output from five different predictor variables from 2070-2099. Used in the same way as the GCM output above. 
* ncep1961-90. NCEP re-analysis data from the five predictor variables from 1961-1990. This data is used to build the statistical model between our local variable and larger scale climate variables. 
* observed1961-90. These are the local variables we are hoping to understand. In this case, we have maximum temperature and precipitation. 

We need to bring the data into our R workspace in a useable form. In R, the most common way to do this is to combine all of our data into one data frame. We achieve this with the blogsville data using the following commands (note that there are many ways to do this):


```{r}
# Obtain a list of all the ncep predictor files
variable_names <- list.files("/home/lee/Dropbox/work/ncar/data/blogsville/ncep1961-90/")
predictor_files <- paste0("/home/lee/Dropbox/work/ncar/data/blogsville/ncep1961-90/", 
                          variable_names)

# Set up a data-frame to store the variables. 
# Do this by reading in the first file and figuring 
# out the number of rows, then allocating a dataframe of that length.     
temp <- read.table(predictor_files[1])
n <- dim(temp)[1]
predictor_matrix <- as.data.frame(matrix(NA, nrow=n, ncol=0))    

# Combine all of the predictor variables into a data frame 
# by looping through each file, reading it in, 
# and binding it (with cbind) to the predictor matrix. 
for (file in predictor_files) {
    single_pred <- read.table(file, stringsAsFactors=FALSE)        
    predictor_matrix <- cbind(predictor_matrix, single_pred)
}

# Rename the variables in the predictor matrix. 
# This will help up later on since it will make the names 
# in all of the different directories uniform
predictor_names <- c("uxx", "vxx", "zxx", "xx500", "humxx")
colnames(predictor_matrix) <- predictor_names

# Finally, read in our predictand variables, get a sequence of 
# dates, and combine these with our predictor matrix. Note that one 
# of the tmax observations comes in as a stange character, so we need 
# to deal with this as a special case. stringsAsFactors = FALSE is just 
# used out of habit, since it usually helps out. You can also set this as the
# default in your R options. 
tmax_predictand <- read.table("/home/lee/Dropbox/work/ncar/data/blogsville/observed1961-90/TMAX.DAT", 
                              stringsAsFactors = FALSE)
colnames(tmax_predictand) <- "tmax"
pcrp_predictand <- read.table("/home/lee/Dropbox/work/ncar/data/blogsville/observed1961-90/PRCP.DAT", 
                              stringsAsFactors = FALSE)
colnames(pcrp_predictand) <- "pcrp"

# WARNING: when downloading this data, in the observed1961-90/TMAX.DAT file,
# line number 10105 currently contains a non numeric symbol. To deal with this, 
# you can either open the file and replace the symbol with "NA", or use the line 
# tmax_predictand[10105,] <- NA after you read in this variable.

# Get a vector of the dates used in the blogsville data. Finally, 
# combine all of these together. 
dates <- seq(as.Date("1960-01-01"), by=1, len = 10957)
blogsville <- cbind(dates, predictor_matrix, tmax_predictand, pcrp_predictand)
head(blogsville)
```
